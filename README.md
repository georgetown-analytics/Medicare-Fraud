# Medicare-Fraud
Cohort 18 Capstone Project for the Certificate of Data Science at Georgetown University School of Continuing Studies.

The expansion of Medicare and Medicaid increased the number of beneficiaries reached through these programs but also increased the opportunity for waste and fraud within the system. The drug prescription, Part D, component of Medicare has a high volume of transactions, leaving them vulnerable to fraud. The Centers for Medicare and Medicaid Services are investigating better methods of using big data to mitigate the effects of fraud and other waste within the system. They collect a lot of data from their claims and post this data publicly for program monitoring. The Department of Health and Human Services Office of Inspector General has authority to exclude providers from Federally funded programs due to fraud and other misdemeanors. The team hypothesized that by connecting claims information from Medicare Part D with excluded providers from the List of Excluded Individuals and Entities (LEIE) database, we could build a machine learning model with features that can predict fraudulent and other improper behavior, such as patient abuse, that would lead to future exclusions. 

The team combined these two datasets through the use of the National Provider Identifier in each to label 465 providers paid for Part D Claims in calendar year 2017 and then excluded from Federally funded health programs within the next three years. Initially the data included 70 possible features describing the number of claims, the number of beneficiaries and drug costs associated with each provider as well as breaking down the information into various classes of the program including the number of brand drug prescriptions or the number of claims filed under low income subsidy. Based on analysis of these features and a review of other Medicare fraud models used in the literature, we incorporated 22 of the original 70 features into our initial models. 

As the target variable we were attempting to predict was exclusion or no exclusion, we applied a supervised learning methodology using binary classification. Initially we did not account for the class imbalance and our models defaulted to predicting the class of no exclusion as there was a 10:1 ratio imbalance. After adjusting for the class imbalance by undersampling as well as scaling our training data, our models provided much more valid results. Out of the ten models we applied, the support vector machine and logistic regression models balanced the Type I and Type II errors the best. As the logistic regression model performed the most consistently based on various size sampling strategies, we determined this as our strongest final model. To increase the simplicity of the model, we also removed the five lowest performing features as they did not have a great effect on the final model.
